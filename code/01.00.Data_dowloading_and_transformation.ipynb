{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Downloading and transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Goals**\n",
    "1. Retrive data from boldsystems.\n",
    "2. Transform the xml files to text files (.tsv).\n",
    "3. Build FASTA format sequnces from the text files.\n",
    "4. Retriving unpublished data from the BOLDSystems and reformating the headers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Retriving data from boldsystems**\n",
    "Using the boldsystemsV4 [PUBLIC DATA API](http://www.boldsystems.org/index.php/resources/api?type=webservices) to export **Full Data Retrieval (Specimen + Sequence)** from a list of countries stored in a file (named by country) in a default destination directory \"co1_metaanalysis/data/input/input_data/bold_africa/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bolddata_retrival() { # This fuction retrives data belonging to a list of country names given. Input can be a file containing names of select countries or idividual country names\n",
    "\n",
    "        usage $@\n",
    "        echo -e \"\\n\\tDownloading data of countries named in $@ from www.boldsystems.org\"\n",
    "\n",
    "        IFS=$'\\n'\n",
    "\n",
    "        for i in `cat $@`\n",
    "        do\n",
    "                wget --show-progress --progress=bar:noscroll --retry-connrefused -t inf -O ${inputdata_path}bold_africa/\"${i}\".xml -a ${inputdata_path}wget_log http://www.boldsystems.org/index.php/API_Public/combined?geo=\"${i}\"&taxon=arthropoda&format=tsv\n",
    "        done\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input error...\n",
      "Usage: bolddata_retrival file1.*[file2.* file3.* ...]\n",
      "\n",
      "\tDownloading data of countries named in  from www.boldsystems.org\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /home/kibet/bioinformatics/github/co1_metaanalysis/code/\n",
    "source ./process_all_input_files.sh\n",
    "bolddata_retrival #country  #Uncomment the word \"country\" to download from a list of countries in the file country (canada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Transformation of the XML files to tsv**\n",
    "Here we use python3 packages : **BeautifulSoup4** and **pandas**.  \n",
    "(**N/B:** Tried using R ([01.02.R_xml_to_tsv.ipynb](./01.02.R_xml_to_tsv.ipynb)), but didn't work well)  \n",
    "For more on the logic behind the extraction script see jupyter notebook [01.01.xml_to_tsv.ipynb](./01.01.xml_to_tsv.ipynb)  \n",
    "The country specific XML files are converted to text (.tsv) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "build_tsv() { #This function generates .tsv files from .xml files using python script and Beautifulsoup4 and pandas package\n",
    "\n",
    "        usage $@\n",
    "\n",
    "        TAB=$(printf '\\t')\n",
    "\n",
    "        echo \"generating .tsv files from .xml downloads\"\n",
    "\n",
    "        for i in \"$@\"\n",
    "        do\n",
    "                if [ ! -f $i ]\n",
    "                then\n",
    "                        echo \"input error: file '$i' is non-existent!\"\n",
    "                elif [[ ( -f $i ) && ( `basename -- \"$i\"` =~ .*\\.(xml) ) ]]\n",
    "                then\n",
    "                        rename\n",
    "                        echo -e \"\\nLet us proceed with file '${input_filename}'...\"\n",
    "                        sed 's/class/Class/g' \"$i\" | sed \"s/$TAB/,/g\" > ${inputdata_path}bold_africa/input.xml\n",
    "                        ${PYTHON_EXEC} ${xml_to_tsv} ${inputdata_path}bold_africa/input.xml && mv output.tsv ${inputdata_path}bold_africa/${output_filename}.tsv\n",
    "                else\n",
    "                        echo \"input file error in `basename -- '$i'`: input file should be a .xml file format\"\n",
    "                        continue\n",
    "                fi\n",
    "        done\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input error...\n",
      "Usage: build_tsv file1.*[file2.* file3.* ...]\n",
      "generating .tsv files from .xml downloads\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /home/kibet/bioinformatics/github/co1_metaanalysis/code/\n",
    "source ./process_all_input_files.sh\n",
    "build_tsv #../data/input/input_data/bold_africa/kenya.xml  #Uncomment the path to execute the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Build FASTA sequences from the .tsv text files**\n",
    "The building of FASTA files is not done directly on the country specific text (.tsv) files.\n",
    "It is done after some cleaning and sorting.\n",
    "1. First only those with Insecta genus-name tag are extracted and all the records are cleaned of any non-COI-5P markers  \n",
    "2. Then ALL the records are re-grouped into subsets based on sequence length  \n",
    "3. Then fourteen 100-record samples are randomly sampled from this groups, to be used in the development and testing of the bioinformatics analysis pipelines  \n",
    "4. Finally the re-grouped subsets and the samples are converted to FASTA format sequences  \n",
    "\n",
    "There are two rscripts:  \n",
    "1. [data_cleanup_n_sampling.R](./data_cleanup_n_sampling.R): Meant for cleaning, sorting and sampling the test data (East African data: Kenya, Tanzania, Uganda, Rwanda, Burundi, Ethiopia and South Sudan).  \n",
    "See [02.00.Data_cleanup](./02.00.Data_cleanup.ipynb) for more information on step '1.' to '3.' \n",
    "2. [data_cleanup.R](./data_cleanup.R): Meant for cleaning and sorting all country specific records\n",
    "\n",
    "**To sort the data for all country specific records into the groups defined by sequence length do as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "clean_sort_tsv() { #This function cleans the .tsv files, sort the records into differnt files based on the sequence length and finally appends this files to a cummulative files of diffent input files\n",
    "\n",
    "        usage $@\n",
    "\n",
    "        echo \"cleaningup and sorting .tsv files \"\n",
    "\n",
    "        output_files_africa=(\"${inputdata_path}clean_africa/afroCOI_500to700_data.tsv\" \"${inputdata_path}clean_africa/afroCOI_650to660_data.tsv\" \"${inputdata_path}clean_africa/afroCOI_all_data.tsv\" \"${inputdata_path}clean_africa/afroCOI_Over499_data.tsv\" \"${inputdata_path}clean_africa/afroCOI_Over700_data.tsv\" \"${inputdata_path}clean_africa/afroCOI_Under500_data.tsv\")\n",
    "\n",
    "        output_files_eafrica=(\"${inputdata_path}clean_eafrica/eafroCOI_500to700_data.tsv\" \"${inputdata_path}clean_eafrica/eafroCOI_650to660_data.tsv\" \"${inputdata_path}clean_eafrica/eafroCOI_all_data.tsv\" \"${inputdata_path}clean_eafrica/eafroCOI_Over499_data.tsv\" \"${inputdata_path}clean_eafrica/eafroCOI_Over700_data.tsv\" \"${inputdata_path}clean_eafrica/eafroCOI_Under500_data.tsv\")\n",
    "\n",
    "\n",
    "        for i in ${output_files_africa[@]}\n",
    "        do\n",
    "                grep \"processid\" $1 > $i && echo -e \"\\nInput file $i is set\"\n",
    "        done\n",
    "\n",
    "        for i in ${output_files_eafrica[@]}\n",
    "        do\n",
    "                grep \"processid\" $1 > $i && echo -e \"\\nInput file $i is set\"\n",
    "        done\n",
    "\n",
    "\n",
    "        for i in \"$@\"\n",
    "        do\n",
    "                if [ ! -f $i ]\n",
    "                then\n",
    "                        echo \"input error: file '$i' is non-existent!\"\n",
    "                elif [[ ( -f $i ) && ( `basename -- \"$i\"` =~ .*\\.(tsv) ) ]]\n",
    "                then\n",
    "                        rename\n",
    "                        echo -e \"\\nLet us proceed with file '${input_filename}'...\"\n",
    "                        ${RSCRIPT_EXEC} --vanilla ${data_cleanup} $i\n",
    "                        case $output_filename in\n",
    "                                Algeria|Madagascar|Angola|Malawi|Benin|Mali|Botswana|Mauritania|Burkina_Faso|Mauritius|Morocco|Cameroon|Mozambique|Cape_Verde|Namibia|Central_African_Republic|Nigeria|Chad|Niger|Comoros|Republic_of_the_Congo|Cote_d_Ivoire|Reunion|Democratic_republic_of_the_Congo|Djibouti|Sao_Tome_and_Principe|Egypt|Senegal|Equatorial_Guinea|Seychelles|Eritrea|Sierra_Leone|Somalia|Gabon|South_Africa|Gambia|Ghana|Sudan|Guinea-Bissau|Swaziland|Guinea|Togo|Tunisia|Lesotho|Liberia|Zambia|Libya|Zimbabwe)\n",
    "                                        input=${inputdata_path}clean_africa/COI_500to700_data.tsv\n",
    "                                        output=${output_files_africa[0]}\n",
    "                                        append_tsvfile\n",
    "\n",
    "                                        input=${inputdata_path}clean_africa/COI_650to660_data.tsv\n",
    "                                        output=${output_files_africa[1]}\n",
    "                                        append_tsvfile\n",
    "\n",
    "                                        input=${inputdata_path}clean_africa/COI_all_data.tsv\n",
    "                                        output=${output_files_africa[2]}\n",
    "                                        append_tsvfile\n",
    "\n",
    "                                        input=${inputdata_path}clean_africa/COI_Over499_data.tsv\n",
    "                                        output=${output_files_africa[3]}\n",
    "                                        append_tsvfile\n",
    "\n",
    "                                        input=${inputdata_path}clean_africa/COI_Over700_data.tsv\n",
    "                                        output=${output_files_africa[4]}\n",
    "                                        append_tsvfile\n",
    "\n",
    "                                        input=${inputdata_path}clean_africa/COI_Under500_data.tsv\n",
    "                                        output=${output_files_africa[5]}\n",
    "                                        append_tsvfile\n",
    "                                        ;;\n",
    "                                *)\n",
    "                                        input=${inputdata_path}clean_africa/COI_500to700_data.tsv\n",
    "                                        output=${output_files_eafrica[0]}\n",
    "                                        append_tsvfile\n",
    "\n",
    "                                        input=${inputdata_path}clean_africa/COI_650to660_data.tsv\n",
    "                                        output=${output_files_eafrica[1]}\n",
    "                                        append_tsvfile\n",
    "\n",
    "                                        input=${inputdata_path}clean_africa/COI_all_data.tsv\n",
    "                                        output=${output_files_eafrica[2]}\n",
    "                                        append_tsvfile\n",
    "\n",
    "                                        input=${inputdata_path}clean_africa/COI_Over499_data.tsv\n",
    "                                        output=${output_files_eafrica[3]}\n",
    "                                        append_tsvfile\n",
    "\n",
    "                                        input=${inputdata_path}clean_africa/COI_Over700_data.tsv\n",
    "                                        output=${output_files_eafrica[4]}\n",
    "                                        append_tsvfile\n",
    "\n",
    "                                        input=${inputdata_path}clean_africa/COI_Under500_data.tsv\n",
    "                                        output=${output_files_eafrica[5]}\n",
    "                                        append_tsvfile\n",
    "                                        ;;\n",
    "                        esac\n",
    "                 else\n",
    "                        echo \"input file error in `basename -- $i`: input file should be a .tsv file format\"\n",
    "                        continue\n",
    "                fi\n",
    "        done\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input error...\n",
      "Usage: clean_sort_tsv file1.*[file2.* file3.* ...]\n",
      "cleaningup and sorting .tsv files \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /home/kibet/bioinformatics/github/co1_metaanalysis/code/\n",
    "source ./process_all_input_files.sh\n",
    "clean_sort_tsv #../data/input_data/bold_africa/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To convert the .tsv files to FASTA format files do as follows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sorting rscript separates East African data from the rest of Africa and stores them in two separate directories: \"co1_metaanalysis/data/input/input_data/clean_eafrica\" and \"co1_metaanalysis/data/input/input_data/clean_africa\"  \n",
    "\n",
    "Below is the code to concatenate the the two different streams into one stored in \"co1_metaanalysis/data/input/input_data/clean_africa\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /home/kibet/bioinformatics/github/co1_metaanalysis/data/input/input_data/\n",
    "cat ./clean_africa/afroCOI_500to700_data.fasta ./clean_eafrica/eafroCOI_500to700_data.fasta #> ./clean_africa/enafroCOI_500to700_data.fasta\n",
    "cat ./clean_africa/afroCOI_650to660_data.fasta ./clean_eafrica/eafroCOI_650to660_data.fasta #> ./clean_africa/enafroCOI_650to660_data.fasta\n",
    "cat ./clean_africa/afroCOI_all_data.fasta ./clean_eafrica/eafroCOI_all_data.fasta #> ./clean_africa/enafroCOI_all_data.fasta\n",
    "cat ./clean_africa/afroCOI_Over499_data.fasta ./clean_eafrica/eafroCOI_Over499_data.fasta #> ./clean_africa/enafroCOI_Over499_data.fasta\n",
    "cat ./clean_africa/afroCOI_Over700_data.fasta ./clean_eafrica/eafroCOI_Over700_data.fasta #> ./clean_africa/enafroCOI_Over700_data.fasta\n",
    "cat ./clean_africa/afroCOI_Under500_data.fasta ./clean_eafrica/eafroCOI_Under500_data.fasta #> ./clean_africa/enafroCOI_Under500_data.fasta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a file called **\"enafroCOI_500to700_data-650to660.fasta\"** of sequences with nucleotide number from 500 to 700, but excluding those with 650 to 660 nucleotides represented in enafroCOI_650to660_data.fasta  \n",
    "Uses a fuctions in a bash script, \"process_all_input_files.sh\", that does the necessary text processing needed.  \n",
    "See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rched for the patterns\n",
    "        # To get the list of orders in description_taxon_names and their frequencies, from  which to select the undesired patterns (names), do: \n",
    "        #grep \">\" seqs.fasta | awk 'BEGIN {FS=\"|\"; OFS=\"|\" ; }; {print $2}' |sort | uniq -c > seqs_orders && less seqs_orders\n",
    "\n",
    "        if [ $# -eq 0 ]\n",
    "        then\n",
    "                echo \"Input error...\"\n",
    "                echo \"Usage: ${FUNCNAME[0]} file1.*[file2.* file3.* ...]\"\n",
    "                return 1\n",
    "\n",
    "        fi\n",
    "\n",
    "        echo -e \"To delete sequences with specific words in the headers please choose [Yes] to proceed or [No] to cancel\"\n",
    "        PS3='Select option YES to delete, [1] or NO to exit, [2]: '\n",
    "        select option in YES NO\n",
    "        do\n",
    "                unset pattern_name\n",
    "\n",
    "                regexp='^[a-zA-Z0-9/_-\\ ]+$'\n",
    "\n",
    "                case $option in\n",
    "                        YES)\n",
    "                                until [[ \"$pattern_name\" =~ $regexp ]]\n",
    "                                do\n",
    "                                        read -p \"Please enter string pattern to be searched:: \" pattern_name\n",
    "                                done\n",
    "\n",
    "                                echo -e \"\\n\\tDeleting all records with description '$pattern_name'...\"\n",
    "\n",
    "                                for i in \"$@\"\n",
    "                                do\n",
    "                                        echo -e \"\\n\\tProceeding with `basename -- $i`...\"\n",
    "                                        rename\n",
    "                                        input_src=`dirname \"$( realpath \"${i}\" )\"`\n",
    "\n",
    "                                        #awk -v name=\"$input_r\" 'BEGIN {RS=\"\\n>\"; ORF=\"\\n>\"}; $0 ~ name {print \">\"$0}' test_all.fasta | less\n",
    "\n",
    "                                        concatenate_fasta_seqs $i\n",
    "                                        $AWK_EXEC -v pattern=\"$pattern_name\" 'BEGIN { RS=\"\\n>\"; ORS=\"\\n\"; FS=\"\\n\"; OFS=\"\\n\" }; $1 ~ pattern {print \">\"$0;}' $i >> ${input_src}/${output_filename}_undesired.fasta\n",
    "                                        sed -i \"/$pattern_name/,+1 d\" $i\n",
    "                                done\n",
    "                                echo -e \"\\n\\tDONE. All deleted records have been stored in '${output_filename}_undesired.fasta'\"\n",
    "                                ;;\n",
    "                        NO)\n",
    "                                echo -e \"Exiting deletion of unwanted sequences...\"\n",
    "                                break\n",
    "                esac\n",
    "        done\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/kibet/bioinformatics/github/co1_metaanalysis/data/input/input_data/\n",
    "source ./process_all_input_files.sh\n",
    "cat enafroCOI_650to660_data.fasta enafroCOI_500to700_data.fasta > #input\n",
    "delete_repeats input\n",
    "x=`wc -l enafroCOI_650to660_data.fasta`\n",
    "awk -v x=$x `{if (NRF<=x) {next} else {print $0} }`./input > enafroCOI_500to700_data-650to660.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting .tsv files into FASTA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "build_fasta() { #This function generates .fasta files from .tsv files using an awk script\n",
    "\n",
    "        usage $@\n",
    "\n",
    "        echo \"generating .fasta files from .tsv metadata files\"\n",
    "\n",
    "        for i in \"$@\"\n",
    "        do\n",
    "                if [ ! -f $i ]\n",
    "                then\n",
    "                        echo \"input error: file '$i' is non-existent!\"\n",
    "                elif [[ ( -f $i ) && ( `basename -- \"$i\"` =~ .*\\.(tsv) ) ]]\n",
    "                then\n",
    "                        input_src=`dirname \"$( realpath \"${i}\" )\"`\n",
    "                        rename\n",
    "                        echo -e \"\\nLet us proceed with file '${input_filename}'...\"\n",
    "                        ${AWK_EXEC} -f ${AWK_SCRIPT} \"$i\" > ${input_src}/${output_filename}.fasta\n",
    "                else\n",
    "                        echo \"input file error in `basename -- $i`: input file should be a .tsv file format\"\n",
    "                        continue\n",
    "                fi\n",
    "        done\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input error...\n",
      "Usage: build_fasta file1.*[file2.* file3.* ...]\n",
      "generating .fasta files from .tsv metadata files\n",
      "Input error...\n",
      "Usage: build_fasta file1.*[file2.* file3.* ...]\n",
      "generating .fasta files from .tsv metadata files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /home/kibet/bioinformatics/github/co1_metaanalysis/code/\n",
    "source ./process_all_input_files.sh\n",
    "build_fasta #../data/input/test_data/*.tsv # For test_data(East African data sets including their samples)\n",
    "build_fasta #../data/input/input_data/clean_africa/*.tsv # For All re-grouped data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Retriving unpublished data from the BOLDSystems and reformating the headers**\n",
    "#### **4.1 To retrive unpublished data from [BOLD Systems](http://www.boldsystems.org/index.php/MAS_Management_UserConsole)**, first create a [BOLD systems account](http://www.boldsystems.org/index.php/MAS_Management_NewUserApp), [login](http://www.boldsystems.org/index.php/Login/page?destination=MAS_Management_UserConsole) and request data managers, to share their data sets.  \n",
    "My list of shared data sets are:\n",
    "1. [DS-KENFRUIT](http://www.boldsystems.org/index.php/MAS_Management_DataConsole?codes=DS-KENFRUIT): managed by Dr Scott E. Miller, has 1,427 records  \n",
    "2. [DS-MPALALEP](http://www.boldsystems.org/index.php/MAS_Management_DataConsole?codes=DS-MPALALEP): managed by Dr Scott E. Miller, has 2,472 records  \n",
    "3. [DS-TBILE](http://www.boldsystems.org/index.php/MAS_Management_DataConsole?codes=DS-TBILE) (Now publicly released now): managed by Dr Scott E. Miller has 90 records  \n",
    "\n",
    "My list of container Projects; these contains multiple data sets within them:\n",
    "4. [IDRCK](http://www.boldsystems.org/index.php/MAS_Management_DataConsole?codes=IDRCK): Has a number of subprojects; IDRC,HIVE, KBIR, KALG, KFISH, KPLA, ARAK and KINS. Has 2,110 sequences (COI-5P=1,704, matK=139, rbcLa=267) out of 6,016 specimen and is managed by Dr. Daniel Masiga.  \n",
    "5. [GMTAH](http://www.boldsystems.org/index.php/MAS_Management_DataConsole?codes=GMTAH),[GMTAI](http://www.boldsystems.org/index.php/MAS_Management_DataConsole?codes=GMTAI) and [GMTAJ](http://www.boldsystems.org/index.php/MAS_Management_DataConsole?codes=GMTAJ) projects. All under the Global Malaise Program and the three have a combined total of 60 Projects and 49246 Specimens.  \n",
    ">1. GMTAH: Has 26 projects titled \"Kenya Malaise Mpala 2014\" with 25,514 specimen, 170 species and 21,742 sequences (COI-5P=21,737, 28S=4 and EF1-alpha=1)  \n",
    ">2. GMTAI: Has 26 projects titled \"Kenya Malaise Kinondo 2014\" with 13,656 specimen, 57 species and 11,805 sequences (COI-5P=11,801, 28S=3 and EF1-alpha=1)  \n",
    ">3. GMTAJ: Has 5 projects titled \"Kenya Malaise Turkana 2014\" with 10,076 specimen, 63 species and 5,175 sequences (COI-5P=5,175)  \n",
    "\n",
    "To retrive this data, I logged into the [BOLD Systems MAS management interface](http://www.boldsystems.org/index.php/MAS_Management_UserConsole) through Chromium web browser and for each named project above: DS-KENFRUIT, DS-MPALALEP, DS-TBILE, IDRCK AND GMTAH-GMTAI-GMTAJ (Mpala_Kinondo_Turkana_Malaise_traps), downloaded the spreadsheet and the sequence files to \"/co1_metaanalysis/data/input/input_data/unpublished\" directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS-KENFRUIT.fasta\n",
      "DS-MPALALEP.fasta\n",
      "idrck.fasta\n",
      "Mpala_Kinondo_Turkana_Malaise_traps.fasta\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /home/kibet/bioinformatics/github/co1_metaanalysis/data/input/input_data/unpublished/\n",
    "ls *.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2 Changing the headers to look uniform to other headers**  \n",
    "Current headers look like:\n",
    ">\\>GMKMV173-15|Cicadellidae|Hemiptera||||Kenya|Mpala Research Centre|0.293|36.899|1650.0  \n",
    ">\\>GMKMW1843-15|Dichomeris tenextrema|Lepidoptera|Dichomeris|Dichomeris tenextrema||Kenya|Mpala Research Centre|0.293|36.899|1650.0  \n",
    "\n",
    "To an edited header that looks like:\n",
    ">\\>GMKMV173-15|Hemiptera|gs-NA|sp-NA|subsp-NA|country-Kenya|exactsite-Mpala_Research_Centre|lat_0.293|lon_36.899|elev-1650.0  \n",
    ">\\>GMKMW1843-15|Lepidoptera|gs-Dichomeris|sp-Dichomeris tenextrema|subsp-NA|country-Kenya|exactsite-Mpala_Research_Centre|lat_0.293|lon_36.899|elev-1650.0  \n",
    "\n",
    "This standardizes the headers to a common format that is useful in the downstream analysis.  \n",
    "For this to be done the headers to a given sequence file are first copied into a file, headers_edit.fasta, within which they are edited to the right format i.e:  \n",
    "1. Deleting the default taxon automatically assigned by BOLD systems during the download process which is usually the lowest taxon defined in the taxonomy of that record.\n",
    "2. Defining the various fields of the headers by adding suffices; gs-\"genus\", sp-\"species\", subsp-\"subspecies\", country-\"country\", exactsite-\"exact site\", lat_\"latitude\", lon_\"longitude\" and elev_\"elevation\".  \n",
    "\n",
    "Then the formated headers are substituted into the actual sequence.fasta file using a function, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "replacing_headers() { #This function takes an input file of edited_fasta_format_headers and searches through a fasta_format_sequence file and substitute their headers if their uniq IDs match\n",
    "        if [ $# -eq 0 ]\n",
    "        then\n",
    "                echo \"Input error...\"\n",
    "                echo \"Usage: ${FUNCNAME[0]} seq.fasta [seq2.fasta seq3.fasta ...]\"\n",
    "                return\n",
    "        fi\n",
    "\n",
    "        unset headers\n",
    "        until [[ ( -f \"$headers\" ) && ( `basename -- \"$headers\"` =~ .*\\.(fasta|fa|afa) ) ]]\n",
    "        do\n",
    "                echo -e \"\\nFor the headers.aln|fasta|fa|afa input provide the full path to the file, the filename included.\"\n",
    "                read -p \"Please enter the file to be used as the FASTA headers source: \" headers\n",
    "        done\n",
    "\n",
    "        echo -e \"\\n\\tStarting operation....\\n\\tPlease wait, this may take a while....\"\n",
    "        for i in \"$@\"\n",
    "        do\n",
    "                unset x\n",
    "                unset y\n",
    "                unset z\n",
    "                echo -e \"\\nProceeding with `basename -- $i`...\"\n",
    "                for line in `cat ${headers}`\n",
    "                do\n",
    "                        #x=$( head -10 idrck_headers | tail -1 | awk 'BEGIN { FS=\"|\"; }{print $1;}') && echo $x\n",
    "                        x=`echo \"$line\" | ${AWK_EXEC} 'BEGIN { RS=\"\\n\"; FS=\"|\"; }{ x = $1; print x; }'`\n",
    "                        y=`echo \"$line\" | ${AWK_EXEC} 'BEGIN { RS=\"\\n\"; FS=\"|\"; }{ y = $0; print y; }'`\n",
    "                        #echo -e \"\\n $x \\n $y\"\n",
    "\n",
    "                        z=`grep \"$x\" $i`\n",
    "                        #echo \"$z\"\n",
    "                        for one_z in `echo -e \"${z}\"`\n",
    "                        do\n",
    "                                if [ $one_z == $y ]\n",
    "                                then\n",
    "                                        echo -e \"Change for ${x} already in place...\"\n",
    "                                        continue\n",
    "                                else\n",
    "                                        echo -e \"\\nSubstituting header for ${x}...\"\n",
    "                                        sed -i \"s/${one_z}/${y}/g\" $i\n",
    "                                        #sed -i \"s/^.*\\b${x}\\b.*$/${y}/g\" $i\n",
    "                                fi\n",
    "                        done\n",
    "                done\n",
    "                echo -e \"\\nDONE replacing headers in `basename -- $i`\"\n",
    "        done\n",
    "        echo -e \"\\n\\tCongratulations...Operation done.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actual run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/kibet/bioinformatics/github/co1_metaanalysis/data/input/input_data/unpublished/\n",
    "source ../../../../code/process_all_input_files.sh\n",
    "for i in $(ls *.fasta2); do grep \">\" $i | sed s/\\|\\|/\\|NA\\|/g | sed s/\\|\\|/\\|NA\\|/g | awk 'BEGIN {FS=\"|\"; OFS=\"|\"} {print $1,$3,\"gs-\"$4,\"sp-\"$5,\"subsp-\"$6,\"country-\"$7,\"exactsite-\"$8,\"lat_\"$9,\"lon_\"$10,\"elev-\"$11}' > headers_edit.fasta; done\n",
    "sed -i 's/\\r$//g; s/ /_/g; s/\\&/_n_/g; s/\\//\\\\&/g' headers_edit.fasta\n",
    "replacing_headers Mpala_Kinondo_Turkana_Malaise_traps.fasta2 << EOF\n",
    "./headers_edit.fasta\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
